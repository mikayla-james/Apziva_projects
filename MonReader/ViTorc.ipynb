{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob"
      ],
      "metadata": {
        "id": "kzpAPDeCCL7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Directory\n",
        "dir = \"/content/drive/MyDrive/Personal/Apziva/MonReader\"\n",
        "\n",
        "# Setting random state for consistency\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Confirm GPU\n",
        "\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "cygJ-SguyGn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2iMdVC0Fvu0"
      },
      "outputs": [],
      "source": [
        "TRAIN_NOT   = f\"{dir}/images/training/notflip\"\n",
        "TRAIN_FLIP  = f\"{dir}/images/training/flip\"\n",
        "TEST_NOT    = f\"{dir}/images/testing/notflip\"\n",
        "TEST_FLIP   = f\"{dir}/images/testing/flip\"\n",
        "\n",
        "def load_paths(pos_dir, neg_dir):\n",
        "    # Use glob.glob to find files directly in the specified directories\n",
        "    neg = glob.glob(os.path.join(neg_dir, \"*\"))\n",
        "    pos = glob.glob(os.path.join(pos_dir, \"*\"))\n",
        "\n",
        "    paths = neg + pos\n",
        "    labels = [0]*len(neg) + [1]*len(pos)\n",
        "    return paths, labels\n",
        "\n",
        "train_files, train_labels = load_paths(TRAIN_FLIP, TRAIN_NOT)\n",
        "test_files,  test_labels  = load_paths(TEST_FLIP, TEST_NOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "\n",
        "model_name = \"google/vit-base-patch16-224\"\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "id2label = {0: \"notflip\", 1: \"flip\"}\n",
        "label2id = {\"notflip\": 0, \"flip\": 1}\n",
        "\n",
        "# Hugging Face Datasets expects dicts\n",
        "train_dict = {\"image\": train_files, \"label\": train_labels}\n",
        "test_dict  = {\"image\": test_files,  \"label\": test_labels}\n",
        "\n",
        "train_ds = Dataset.from_dict(train_dict)\n",
        "test_ds  = Dataset.from_dict(test_dict)\n",
        "\n",
        "def preprocess_images(examples):\n",
        "    images = [Image.open(p).convert(\"RGB\") for p in examples[\"image\"]]\n",
        "    # Don't specify return_tensors - let it return numpy/lists\n",
        "    inputs = processor(images=images)\n",
        "    return {\"pixel_values\": inputs[\"pixel_values\"], \"labels\": examples[\"label\"]}\n",
        "\n",
        "train_ds = train_ds.map(\n",
        "    preprocess_images,\n",
        "    batched=True,\n",
        "    batch_size=32,  # Much smaller chunks\n",
        "    remove_columns=[\"image\"],\n",
        "    writer_batch_size=100  # Flush to disk more often\n",
        ")\n",
        "\n",
        "test_ds = test_ds.map(\n",
        "    preprocess_images,\n",
        "    batched=True,\n",
        "    batch_size=32,\n",
        "    remove_columns=[\"image\"],\n",
        "    writer_batch_size=100\n",
        ")\n"
      ],
      "metadata": {
        "id": "Ry81xZKUF1PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True # Add this argument to ignore size mismatches in the classification head\n",
        ")\n",
        "# For up-to-date versions\n",
        "#   eval_strategy = \"epoch\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{dir}/vit-flip-checkpoints\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "DtUNF3JVF4SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "pred_output = trainer.predict(test_ds)\n",
        "logits = pred_output.predictions\n",
        "labels = pred_output.label_ids\n",
        "preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(labels, preds))\n",
        "print(\"F1:\", f1_score(labels, preds, average=\"weighted\"))\n"
      ],
      "metadata": {
        "id": "DIghdqawGG7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 datasets:\n",
        "1. Test images both flipped and not flipped (n = 100)\n",
        "2. Images ViT predied as not flipped (n=100)"
      ],
      "metadata": {
        "id": "bsfX_gHzOvhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions from your existing trainer\n",
        "pred_output = trainer.predict(test_ds)\n",
        "logits = pred_output.predictions\n",
        "label_ids = pred_output.label_ids  # true labels, if you need them\n",
        "\n",
        "# For binary classification with 2 logits per sample:\n",
        "preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "# If your model outputs a single logit (sigmoid), do:\n",
        "# preds = (logits.squeeze(-1) > 0).astype(int)\n",
        "\n",
        "# Indices of images predicted as \"not flipped\" (assume label 0)\n",
        "non_flipped_idx = np.where(preds == 0)[0]\n",
        "\n",
        "# Create a subset dataset with only predicted non-flipped images\n",
        "non_flipped_ds = test_ds.select(non_flipped_idx)\n",
        "\n",
        "print(f\"Total images in test_ds: {len(test_ds)}\")\n",
        "print(f\"Images predicted as not flipped: {len(non_flipped_ds)}\")\n"
      ],
      "metadata": {
        "id": "s6xKlxlpO2qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ocr_pipe = pipeline(\n",
        "    \"image-to-text\",\n",
        "    model=\"microsoft/trocr-base-printed\",\n",
        "    device=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "-lcx3_6cCxp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from math import ceil\n",
        "\n",
        "def run_ocr_and_time(dataset, batch_size=8):\n",
        "    \"\"\"\n",
        "    dataset: HF Dataset with an 'image' column (PIL images or arrays)\n",
        "    Returns: (total_time_seconds, texts_list)\n",
        "    \"\"\"\n",
        "    n = len(dataset)\n",
        "    all_texts = []\n",
        "    start = time.perf_counter()\n",
        "\n",
        "    # Loop in batches\n",
        "    for i in range(0, n, batch_size):\n",
        "        batch = dataset[i:i+batch_size]\n",
        "        images = batch[\"image\"]  # list of images\n",
        "        # OCR pipeline supports list input\n",
        "        outputs = ocr_pipe(images)\n",
        "\n",
        "        # outputs is typically a list of dicts like [{'generated_text': '...'}, ...]\n",
        "        texts = [o[\"generated_text\"] for o in outputs]\n",
        "        all_texts.extend(texts)\n",
        "\n",
        "    end = time.perf_counter()\n",
        "    return end - start, all_texts\n",
        "\n",
        "# 3a. Time on full test set\n",
        "time_full, texts_full = run_ocr_and_time(test_ds, batch_size=8)\n",
        "print(f\"OCR time on FULL dataset ({len(test_ds)} images): {time_full:.2f} seconds\")\n",
        "\n",
        "# 3b. Time on predicted non-flipped subset\n",
        "time_nonflip, texts_nonflip = run_ocr_and_time(non_flipped_ds, batch_size=8)\n",
        "print(f\"OCR time on NON-FLIPPED subset ({len(non_flipped_ds)} images): {time_nonflip:.2f} seconds\")\n",
        "\n",
        "# Optional: relative speedup\n",
        "speedup = time_full / time_nonflip if time_nonflip > 0 else float(\"inf\")\n",
        "print(f\"Speedup from filtering (full / non-flipped): {speedup:.2f}x\")\n"
      ],
      "metadata": {
        "id": "9T_BmUEdIJ9j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}